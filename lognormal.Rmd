---
title: "lognormal"
author: "Steve Simon"
date: "May 3, 2017"
output: html_document
---

```{r preliminaries, echo=FALSE, message=FALSE, warning=FALSE}
# library(broom)
# library(cowplot)
library(dplyr)
library(ggplot2)
# library(gridExtra)
library(knitr)
library(magrittr)
# library(quantreg)
library(rstan)
# library(tidyr)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
opts_chunk$set(
  echo=TRUE,
  message=FALSE,
  warning=FALSE)
```

I wanted to use the lognormal distribution to simulate a random effect that is multiplicative, but it is a bit tricky to do this in practice. I needed to review some fundamental properties of the lognormal distribution to get it right.

The lognormal distribution is a distribution where the log is normal. More formally, X is lognormally distributed if log(X) is normally distributed. Inverting that function, you can also say that if Y is normally distributed, then exp(Y) is lognormally distributed.

While the normal distribution is symmetric, the lognormal distribution is skewed to the right (skewed positive), meaning that it tends to produce has outliers on the right more often than on the left. If the value of $\sigma$ in the normal distribution is large, the lognormal distribution will end up extrememely skewed.

You can generate a random sample of lognormal variables in two different ways. You can compute a random sample of normal variables (using rnorm) and then exponentiating the values, or you can generate the lognormal values directly using the rlnorm function.

```{r simple}
y <- rnorm(1000, 0, 1)
x1 <- exp(y)
lb1 <- "exp(rnorm)"
x2 <- rlnorm(1000, 0, 1)
lb2 <- "rlnorm"
data.frame(x=c(x1, x2), lb=c(lb1, lb2)) %>%
  ggplot(aes(lb, x))                     +
  ylab(" ")                              + 
  xlab(" ")                              +
  geom_boxplot()                         +
  stat_summary(fun.y="mean", 
               geom="point", 
               size=4,
               pch="+")                  +
  coord_flip()
```

Notice that even with a standard deviation of only 1, the lognormal distribution is quite skewed.

Like most other packages, R defines the lognormal distribution in terms of the parameters $\mu$ and $\sigma$ of the underlying normal distribution. But you can also define the lognormal distribution in terms of the geometric mean (GM) and the geometric standard deviation (GSD) with

GM = $e^\mu$ and GSD = $e^\sigma$.

Just for reference, since you know the probability density function of the normal distribution, it is not hard to calculate the probability density function of the lognormal distribution.

The fundamental change of variable approach requires a monotone function (or requires a few tedious corrections). Thankfully exp() is a monotone function. The general result is that for a random variable Y=g(X) where X has pdf $f_X(x)$, the density function of Y, $f_Y(y)$, is equal to

$\lvert {d \over dy} g^{-1}(y) \rvert f_X(g^{-1}(y))$ 

The first half of this expression is the dreaded Jacobian. But since $g^{-1}$ is just the natural logarithm, the first half end up being $1 \over y$. The density of the normal distribution is

${1 \over \sigma~\sqrt{2\pi}} e^{-{(x-\mu)^2}/(2~\sigma^2)}$

so just replace x with log(y) and stick in the Jacobian $1 \over y$ to get

${1 \over y~\sigma~\sqrt{2\pi}} e^{-{(log(y)-\mu)^2}/(2~\sigma^2)}$

I mention this because that Jacobian is needed at times with Bayesian models.

Here's what the density function looks like for a lognormal(0, 1) distribution.

```{r density1}
x <- seq(0, 10, length=1000)
y <- dlnorm(x, 0, 1)
data.frame(x, y) %>%
  ggplot(aes(x, y)) +
  xlab(" ") +
  ylab("lognormal(0,1) density") +
  geom_line()
```

The parameter $\mu$ in a lognormal distribution is referred to as a scale parameter, and the parameter $\sigma$ is often called a shape parameter. This seems wrong. In the normal distribution, $\sigma$ is the scale parameter and there is no shape parameter for the normal distribution. But things change when you convert the normal distribution to the lognormal distribution through the exponential function. Recall that $\mu$ + N(0, $\sigma$) has the same distribution as N($\mu$, $\sigma$). So it isn't too hard to show that lognormal($\mu$, $\sigma$) has the same distribution as $e^\mu$ lognormal(0, $\sigma$).

Compare the densitiy of the lognormal(0, 1) shown above to the density of the lognormal(2, 1).

```{r density2}
x <- seq(0, 10, length=1000)
x <- x*exp(2)
y <- dlnorm(x, 2, 1)
data.frame(x, y) %>%
  ggplot(aes(x, y)) +
  xlab(" ") +
  ylab("lognormal(2,1) density") +
  geom_line()
```

The two distributions look identical, except for the tick marks on the X axis. That's because I carefully expanded the range of the X-axis by exp(2).

So how does the scale parameter $\sigma$ from a normal distribution become a shape parameter in a lognormal distribution? The exponential function is curved and ends up stretching the high values of the normal distribution and squeezing the low values of the normal distribution. This creates right skew. How much skew depends on how much variation there is in the underlying normal distribution. If the underlying normal distribution has a small standard deviation, producing a small effective range, the exponential function isn't too much different from a linear functiton. So this lognormal doesn't skew very much. The curvature of the exponential functiton becomes more pronounced as standard deviation increases, producing more extreme skews.

Here is the density function for a lognormal distribution with $\sigma$ = 0.1

```{r density3}
sigma.list <- c(0.1, 0.2, 0.5, 1)
lb <- paste("sigma =", sigma.list)
x <- rep(seq(0, 5, length=1000), 4)
sigma <- rep(sigma.list, each=1000)
y <- dlnorm(x, 0, sigma)
data.frame(x, y, sigma) %>%
  ggplot(aes(x, y)) +
  xlab(" ") +
  ylab("lognormal density") +
  geom_line() +
  facet_grid(sigma~., scales="free_y", labeller=label_both)
```

So my thought was to generate a single rate based on the gamma distribution,

$\lambda$ ~ Gamma(N*S, N*T),

where N, S, and T are constants provided elsewhere. Then you create lognormal disturbances

$\eta_i$ ~ lognormal(0, $\sigma$)

and compute rates for each center in a multi-center trial as

$\lambda~\eta_i$

With M centers, I computed a count for each center that was distributed as

Poisson($\lambda~\eta_i~T / M$)

Fair enough. By making the lognormal distribution have a mean of zero on the underlying normal scale, that's the same (so I thought) to having a mean of 1 on the lognormal scale. A large value of $\sigma$ would produce large variations in the rates from one center to another and a small value of $\sigma$ would produce more consistency in the rates across centers. The second half I got right. But what I failed to recognize is that an increase in $\sigma$ would also lead not only to greater dispersion in rates but also to an increase in the overall average rate.

Start with a relatively small amount of variation between sites, a GSD=1.2. The earlier simulations using a single site showed a predicted total sample size of about 350 patients. A multicenter study with GSD=1.2 appears to produce a predicted total sample size that is also around 350 patients on average.

```{r lognormal1a, fig.height=1, fig.width=5}
f <- "lognormal1.stan"
da <- list(N=350, T=3*365, S=0.5, M=10,
  S1=0.5, GSD=1.2)
fit_ln1a <- stan(file=f,
  data=da, iter= 1000, chains = 4)

fit_ln1a                                 %>%
  as.data.frame                          %>%
  mutate(i="GSD=1.2")                    -> sim_ln1a
sim_ln1a                                 %>%
  ggplot(aes(i, Nstar))                   +
  expand_limits(y=0)                      +
  ylab("Estimated total sample size")     + 
  xlab(" ")                               +
  geom_boxplot()                          +
  stat_summary(fun.y="mean", 
               geom="point", 
               size=4,
               pch="+")                  +
  coord_flip()
```

When the GSD increases to 1.5, you see a lot more variation in the estimated total sample size, but also a bit larger.

```{r lognormal1b, fig.width=5, fig.height=1.33}
f <- "lognormal1.stan"
da <- list(N=350, T=3*365, S=0.5, M=10,
  S1=0.5, GSD=1.5)
fit_ln1b <- stan(file=f,
  data=da, iter= 1000, chains = 4)

fit_ln1b                                 %>%
  as.data.frame                          %>%
  mutate(i="GSD=1.5")                    -> sim_ln1b
sim_ln1b                                 %>%
  bind_rows(sim_ln1a)                    %>%
  ggplot(aes(i, Nstar))                   +
  expand_limits(y=0)                      +
  ylab("Estimated total sample size")     + 
  xlab(" ")                               +
  geom_boxplot()                          +
  stat_summary(fun.y="mean", 
               geom="point", 
               size=4,
               pch="+")                  +
  coord_flip()
```

Increase the GSD again to 2.0 and you have an even larger estimated total sample size.

```{r lognormal1c, fig.width=5, fig.height=1.67}
f <- "lognormal1.stan"
da <- list(N=350, T=3*365, S=0.5, M=10,
  S1=0.5, GSD=2.0)
fit_ln1c <- stan(file=f,
  data=da, iter= 1000, chains = 4)

fit_ln1c                                 %>%
  as.data.frame                          %>%
  mutate(i="GSD=2")                      -> sim_ln1c
sim_ln1c                                 %>%
  bind_rows(sim_ln1b)                    %>%
  bind_rows(sim_ln1a)                    %>%
  ggplot(aes(i, Nstar))                   +
  expand_limits(y=0)                      +
  ylab("Estimated total sample size")     + 
  xlab(" ")                               +
  geom_boxplot()                          +
  stat_summary(fun.y="mean", 
               geom="point", 
               size=4,
               pch="+")                  +
  coord_flip()
```

This is not good. A large amount of variation in accrual rates from center to center should not be an advantage. Maybe it hurts, and maybe it's neutral, but there's no rational basis for believing that large variation in accrual rates helps speed up accrual.

So what's going on here? The geometric mean of a lognormal distribution is the median of the of the lognormal distribution, but for a skewed right distribution, the mean is larger than the median. You could compute how much more the mean is larger, but it's a bunch of tedious calculations, so let me just show you the final answer:

$E[Y] = e^{\mu+0.5\sigma^2}$

So this suggests a solution to our problem. Set $\mu$ to $-0.5\sigma^2$ instead of 0. Changing $\mu$ effectively rescales the lognormal distribution so that the arithmetic mean of the lognormal distribution is always 1.

```{r lognormal2a, fig.width=5, fig.height=1.67}
f <- "lognormal2.stan"
da <- list(N=350, T=3*365, S=0.5, M=10, GSD=1.2)
fit_ln2a <- stan(file=f,
  data=da, iter= 1000, chains = 4)
fit_ln2a                                 %>%
  as.data.frame                          %>%
  mutate(i="GSD=1.2")                    -> sim_ln2a

da <- list(N=350, T=3*365, S=0.5, M=10,
  S1=0.5, GSD=1.5)
fit_ln2b <- stan(file=f,
  data=da, iter= 1000, chains = 4)

fit_ln2b                                 %>%
  as.data.frame                          %>%
  mutate(i="GSD=1.5")                    -> sim_ln2b

da <- list(N=350, T=3*365, S=0.5, M=10,
  S1=0.5, GSD=2.0)
fit_ln2c <- stan(file=f,
  data=da, iter= 1000, chains = 4)

fit_ln2c                                 %>%
  as.data.frame                          %>%
  mutate(i="GSD=2")                      -> sim_ln2c
sim_ln2c                                 %>%
  bind_rows(sim_ln2b)                    %>%
  bind_rows(sim_ln2a)                    %>%
  ggplot(aes(i, Nstar))                   +
  expand_limits(y=0)                      +
  ylab("Estimated total sample size")     + 
  xlab(" ")                               +
  geom_boxplot()                          +
  stat_summary(fun.y="mean", 
               geom="point", 
               size=4,
               pch="+")                  +
  coord_flip()
```


This produces a series of lognormal distributions where the arithmetic mean is 1.0 regardless of how big or how small $\sigma$ is. Notice that the median is slightly smaller for larger values of the GSD. If you want to make sure that both the mean AND the median of the total estimated sample size are unaffected by the GSD, you're out of luck.

Now, I have worked a lot with the lognormal distribution in the past, so I should have known this, but it's easy to overlook things when you're in a rush and there are complications pouring in from all directions.

You could develop an accrual model where the center variations follow a different distribution. A Gamma distribution, for example where the $\alpha$ and $\beta$ parameters are constrained to be equal, for example, is a reasonable alternative.
